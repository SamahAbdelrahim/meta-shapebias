---
title             : "Examining the robustness and generalizability of the shape bias: A meta-analysis"
shorttitle        : "Meta-analysis of the shape bias"

author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Second Author"
    affiliation   : "1,2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Some-University"
  - id            : "2"
    institution   : "Some-School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.
  
abstract: |
  The "shape bias"—the tendency to generalize new nouns by their shape rather than features like color or texture—is considered a cornerstone of early noun learning. However, the magnitude, development, and cross-cultural generality of this bias remain debated. We conducted a pre-registered meta-analysis and meta-regression of 71 studies to synthesize the evidence base. Our analysis revealed a robust overall effect, confirming the existence of a shape bias. However, this effect was characterized by extreme between-study heterogeneity (I² = 0.95). Critically, meta-regression analyses failed to support hypothesized developmental changes, with a constant model across ages providing the most parsimonious fit. The literature is overwhelmingly dominated by studies of English-speaking children, preventing meaningful tests of cross-linguistic or cross-cultural variation. We also found strong statistical evidence of publication bias, suggesting that the true effect size could be inflated in the published literature. Together, these findings indicate that procedural variation, besides a potential publication bias, more than currently testable theoretical moderators, account for the variability in the literature, severely limiting the ability to distinguish between theories of the shape bias's origins. 
  
keywords          : "Shape bias; Word learning; Developmental change; Cross-cultural differences; Meta analysis; Publication bias; Statistical over-generalization"
wordcount         : "3930"

bibliography      : ["r-references.bib", "meta-shapebias.bib", "library.bib" ]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
#output            : papaja::apa6_docx

always_allow_html: true
csl: apa6.csl
toc: false 
figsintext: yes
fig_caption: yes

header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{graphicx}
---



```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.crop = TRUE, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=FALSE, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(papaja)
library(knitr)
#remotes::install_github("cran/PRISMAstatement")
library(PRISMAstatement)
library(metafor)
library(here)
library(broom)
library(kableExtra)
#remotes::install_github("MathiasHarrer/dmetar")
library(dmetar)
library(DiagrammeR)

```

# Introduction

If a pre-linguistic child hears the word "spoon" in the presence of a spoon, how could they know whether the word refers to the whole object, its color, just the bowl but not the rest of the spoon, or this specific spoon but not other spoon-like objects? Despite the large number of logically possible hypotheses about the meaning of each individual word, children learn word meanings quickly [@quine1954]. Additionally, children are few-shot learners, and without explicit instruction or feedback, they are able to generalize the meaning of a word to new members of that semantic category that share some properties with the original referent. For example, if a child hears the word "spoon" in the presence of a spoon, they can generalize this word to other spoons, but not to forks or knives. To resolve this puzzle, researchers have proposed a variety of innate constraints, learned biases, and reasoning processes that could potentially help children identify word meanings, and successfully extend them to category members. One well-studied proposal is the idea that children have a *shape bias*: a tendency to generalize nouns by the referent's shape more than other perceptual attributes like color, texture, or size [@landau1988; @subrahmanyam_2006] or conceptual attributes like a functional, thematic, or a taxonomic match [@imai_childrens_1994; @Poulin-Dubois1999; @imai2010; @yoshida2003known]. 

One reason for researchers' long-standing interest in the shape bias relates to its hypothesized role in early word learning. In one study, @smith_object_2002 trained 17-month-old children over 7 weeks by repeatedly playing with and hearing names of unfamiliar objects from categories that were well organized by shape. Children who received the training showed evidence of generalizing new categories by shape compared with children who hadn't received the training, suggesting that practice with shape categories had helped them acquire a broader, more general shape bias. Further, the trained group learned more object names -- as measured by parent reports -- during and after the intervention, suggesting that the shape bias could be an important tool for noun learning for children in this age group [see [@Perry2010_Variability] for further discussion]. Additionally, it has been found to be weaker in children with language impairments, and in children with autism spectrum disorder [@JONES_SMITH_2005; @Jones2003LateTS; @Collisson2015IndividualDI; @perry2019heterogeneity; @potrzeba2015_ASD; @tek2008children; @perry2021vocabulary]. Although the causality direction is not yet clear, but this suggests that the shape bias may be a useful tool for children learning nouns, and that it may be a useful predictor of language development. 

The shape bias is typically demonstrated in different varieties of word extension tasks. In their basic setup, these tasks typically involve giving a novel label to a single exemplar object, which is then contrasted with test objects that vary in shape, color, texture, or other properties. For instance, in one study 15-month-old children were shown an exemplar object that had a specific non-visual property like producing a sound when tapped. Then children were asked which of the test objects – which matched the exemplar in either color, shape, or material – belonged to the same category. Children selectively extended the non-visual property to the test objects that resembled the exemplar in shape, revealing a bias towards this dimension [@graham_2010; @specific_bloom; @samuelson_dynamic_2007]. 

Yet the degree of shape bias observed in word learning experiments varies across ages, cultures, languages, experimental protocols, and items. Some of this variation has been claimed to be theoretically important -- for example, cross-cultural and developmental variation -- while some is likely due to procedural details, or random statistical variation. In addition, some variation may be caused by publication bias -- the practice of selectively publishing results that achieve statistical significance -- which has been an issue in other developmental literatures [@bergmann2018;@frank2017]. The current paper aims to synthesize evidence relating to the shape bias, using statistical meta-analysis to quantify its overall effect across different studies, different sources of variation in its magnitude, and to consider links between these sources of variation and potential theoretical accounts.

Here we use the term "bias" as a convenient descriptor of this behavioral phenomenon without presupposing any particular underlying mechanism. Although the underlying mechanism of the phenomenon, i.e. whether it is an automatic attentional mechanism, or a more conceptually regulated process "shape as a cue" is also debated, we do not attempt to resolve this debate in the current paper. Additionally, there seems to be a lack of explicitly shared definition of what characterizes a "shape", whether it involves the skeleton, topology, axes, parts, or the boundaries of an object (More discussion on these variations below). We also don't attempt to provide a specific definition here. Rather we ask how much evidence we have on the robustness of the behavior, how it varies, and whether we have moderation power to test for theoretical accounts of the shape bias at the phenomenon level. 

## Theoretical accounts of the shape bias

Where does the shape bias come from? We consider four different explanations of this behavioral tendency: innateness, learning from lexical statistics, learning from syntax, and learning from environmental statistics. These explanations are not necessarily mutually exclusive -- multiple sources could contribute to the developmental emergence of the shape bias. 

One explanation for the shape bias could be that children have an innate tendency to generalize by shape. This proposal was called into question, however, by the observation that the shape bias is primarily found in linguistic generalization tasks as opposed to non-linguistic similarity judgment tasks, indicating that it is unlikely to be a generalized attentional bias [@landau1988; @smith_object_2002; @poulin1999role; @smith1992Adjective; @samuelson2000children; @imai_childrens_1994; @dumb_mechanism1996] [see @specific_bloom for counter argument]. Further, the magnitude of the shape bias appears to vary across ages, language onset, cultures, and languages, inconsistent with a purely nativist explanation.

An alternative proposal argues that children acquire the shape bias by generalizing the lexical statistics of their language. The early vocabulary of English-speaking children in the US is predominantly comprised of nouns referring to solid objects, suggesting that the shape bias may be an "over-hypothesis" that nouns tend to generalize in the same way as one another [@gershkoff2004shape; @samuelson_statistical_2002; @samuelson1999; @perry2010learn; @colunga2000learning; @yoshida2003known; @perry2011; @dumb_mechanism2004; @perry2021vocabulary]. Children's early experience with nouns referring to solid and countable objects, in turn, privileges the property of shape, leading to an inductive bias, such that as children learn more nouns, they should be more likely to pick up on shape as an organizing regularity of the lexicon. This proposal is supported by the observation that both children and computer simulations trained with larger US English vocabulary show a stronger shape bias [@samuelson_statistical_2002; @samuelson1999; @perry2011shape] [see [@spanish_CANTRELL_2012; @colunga2017not; @tek2008children; @perry2021vocabulary; @graham1999infants; @booth2005conceptual; @animate_graham1999] for further discussion and counter arguments]. However, this proposal alone, lacks a quantifiable robust evidence for why children learning different languages (e.g., East Asian languages) show a weaker shape bias, and how their capacity to learn nouns develop. 

Additionally, this generalization may be guided by the syntax of children's native language. For example, The syntactic marking that distinguishes count and mass noun is not a universal aspect of languages. In English, count nouns (e.g., "ball") are generalized by shape while mass nouns (e.g., "sand") that cannot be modified by numerals or combined with indefinite articles ("a" or "an") are not. The presence of this distinction might facilitate English-speaking children (and other children learning languages with a mass-count distinction) in picking out the category of words to which the shape bias can be productively applied. In contrast, children learning languages that lack the mass-count distinction (e.g., some East Asian languages like Mandarin or Japanese) do not have access to this organizing principle. Some experiments have explicitly varied whether count/mass language is used in the presentation of new objects, directly testing whether this syntactic cue guides generalization [@samuelson2008rigid; @soja1991ontological; @soja_perception_1992; @tran2012honoring]. Theoretically, this syntactic facilitation could take three routes; it might build the structure of thought that underlies perceiving and categorizing objects from scratch. Secondly, it might provide cues that lead to carving out the existing objects space and restructuring it. Thirdly, it might provide cues that develop sensitives that respect the statistical regularities of both the linguistic and perceptual space without modifying it, making the statistics of the lexicon less clear cut for classifier language speakers. Here, we are not interested in theoretically disentangling these three routes, as they could all lead to the same behavior, but rather to quantify the degree to which the shape bias is moderated by the presence of mass/count syntax in the language used in the experiment. 

A final proposal is that shape bias might in part be a function of exposure to manufactured artifacts, since children often see many examples of an artifact that vary in color, size, or texture but are similar in shape (e.g., balls or toy cars). Exposure to such artifacts is higher in industrialized cultures where a built environment is more prevalent. While nearly all hammers are hammer-shaped, many plants are similar to one another in overall shape but vary in other details such as color, texture, or sub-part shape. Children in more industrialized cultures might see relatively more hammer-like categories and relatively fewer plant-like categories compared with children in less-industrialized contexts. Supporting this hypothesis, Tsimane' speakers in Bolivia, who have relatively less artifact experience, showed a lower level of shape bias compared with English speakers from an industrialized culture [@jara2022]. Once again, how much similarity or distinction exists between the statistical regularities in the lexicon and the statistical regularities in the environment is not clear. 

This work aims to quantify how much empirical support there is for each of these theories, and how much of the variation in the shape bias can be explained by the constructs relevant to each theoretical account. 

## Dimensions of variation in the shape bias

Across the large literature that investigates children's shape-based generalizations, the magnitude of the observed shape bias varies widely. We next review some of these sources of variation, which guide the research questions that we assess using our meta-analysis. Some -- but not all -- sources of variation relate to the theoretical proposals about the origins of the shape bias. These links between observed variation and theory are not always clear and have rarely been stated in quantitative form. In the Discussion, we revisit the question of how to link our meta-analysis to the theoretical accounts described above.

First, the shape bias varies developmentally, but the trajectory of the change has not been mapped precisely. For example, the magnitude of the shape bias appears to emerge at or before the second birthday. In some studies it continues to increase at the 3rd year until adulthood [@samuelson2008rigid; @landau1988; @jara2022] showing a positive trend with age. While it decreases in some others. For example, in one study, 3-year-olds showed a strong bias to generalize nouns by shape, while 5-year-olds and adults showed a weaker bias [@landau1998object; @subrahmanyam_2006;@imai_childrens_1994]. These developmental trajectory are closely related to theoretical accounts of shape bias, and each form tells a different story. For example, a quadratic developmental curve was used in previous studies to support the notion that shape bias is a statistical over-generalization built over the associations children draw between the type of objects encountered in their early life, and how those objects are labeled (a noun bias), and syntactic markings (syntactic bootstrapping), resulting in a heuristic that facilitate the process of expanding vocabulary. Then, they undergo a hypothesis shift with increasing sensitivity to other category cues, producing a curvilinear trajectory [@samuelson2008rigid]. Age differences were also a function of the specifics of the task, condition, and items used [@imai1997; @graham1999function; @dumb_mechanism1996; @booth2005conceptual;@Subrahmanyam1999_age_complex]. In our meta-analysis, we attempt to quantify developmental change in the magnitude of the bias across different studies (Research Question 1).

Second, studies on the shape bias have been conducted across a variety of languages and cultures that vary in their mass/count syntax, other linguistic properties, and many cultural dimensions including environment and industrialization. Speakers of Eastern Asian languages such as Japanese and Mandarin have been claimed to show a reduced tendency to rely on shape for word extension [@gathercole_1997; @imai1997; @jara2022; @samuelson1999; @soja1991ontological; @subrahmanyam_2006; @yoshida2003; @imai2010; @tran2012honoring; @CANTRELL2013258]. The intersection of cross-linguistic and developmental variation adds another layer of complexity. In one study, English- and Mandarin-speaking 3-year-olds generalized by shape. In contrast, Mandarin-speaking 4-year-olds and adults generalized by material while English-speaking 4-year-olds and adults showed a preference for shape [@subrahmanyam_2006]. As is common in cross cultural/cross-linguistic research, observed variation across populations can be difficult to interpret due to the myriad correlated cultural and linguistic factors that differ between populations. We attempt to quantify evidence for cross-cultural and cross-linguistic differences across ages (Research Question 2).

Third, studies vary significantly in their procedures, stimuli, and comparison dimensions. Stimuli in studies range from real objects to animate categories, artifacts, drawings, silhouettes, and pictures. For example, two-year-olds were found to generalize by shape in the case of artifacts, while relying on both shape and texture in the case of animate categories [@booth2005conceptual; @yoshida2003] (see @animate_graham1999 for counter argument). Likewise, shape bias was found to be more prevalent with three dimensional objects than two dimensional ones [@davidson2018effects]. Procedural variation is also important: in another study, the shape bias was absent when children were not forced to choose between test objects [@cimpian2005absence]. These studies highlight variability due to relatively minor aspects of stimulus representation or instructions.

The shape bias is thought to be a distinctive extension strategy for solid objects, but it is also reported to be mediated by the complexity of the stimuli, which in turn often covaries with the perception that objects have a specific function. When they were directly given information about objects' function, preschool children use shape to generalize the noun label, while older children are more likely to use function [@graham1999; @gentner_1978; @smith_naming_1996; @landau1998object]. However, results are more varied for studies where complexity rather than function is manipulated. For example, in some studies, English and Japanese-speaking children were found to prioritize shape for complex industrial artifacts with clear functions but only Japanese children, however, were claimed to show reduced shape generalization for simple uniform objects with no clear function [@imai1997; @gentner_1978; @srinivasan2019_polysemy; @Sandhofer2004_complex; @cook2006_bilingual_complex]. In contrast, in other studies the complexity of objects elicited less variation in shape bias [@cimpian2005absence; @potrzeba2015_ASD].^[The notion of object complexity doesn't have a clear and explicitly shared definition. In addition, function and complexity are used interchangeably in the literature. For example, in one study, the authors used the term "complex" to refer to industrial objects with a clear function, while in other studies, the same term was used to refer to objects' number of parts, corners, boundaries, curvature, and richness of details, regardless of its industrial and functional status [@imai1997; @gentner_1978; @SON2008_complex; @Subrahmanyam1999_age_complex].] Thus, we also attempt to evaluate the relationship between complexity of objects and the propensity to generalize by shape (Research Question 3).

```{r prisma, fig.height = 3, fig.width =4, echo=FALSE, fig.cap="PRISMA diagram showing the results of our literature screening process.", fig.align = "center", fig.pos = "H"}
knitr::include_graphics("prisma.png")

```
Finally, the syntax used in the presentation of stimuli varies across experiments. Some experimental procedures explicitly use mass/count marking to manipulate generalization, while other procedures avoid syntactic marking and use neutral syntax instead (using "my/this dax", instead of "a dax" or "some dax") [@soja1991ontological; @imai1997; @samuelson2008rigid; @soja_perception_1992; @perry2011shape; @horst2013s; @dansereau2017young]. Again, sensitivity to mass/count syntax might vary developmentally; in some studies, younger children showed more sensitivity to syntax that is incongruent with the target [e.g., using count syntax with substances; @soja_1992]. We attempt to quantify the effects of syntax and its interaction with development (Research Question 4).

## The current study

Given the wide range of factors claimed to influence the shape bias, as well as the disparities between studies in their samples and methods, we were interested in consolidating the current base of knowledge. A previous review of the shape bias provides an important qualitative synthesis of the evidence base, and has been essential for mapping themes, methods, and interpretive debates [@KUCKER2019156]. To our knowledge, however, there has been no systematic literature search combined with a quantitative synthesis. We believe a quantitative meta-analysis is necessary to provide quantitative estimates of effect magnitude, variance, or bias in the cumulative literature. Also, only a quantitative meta-analysis can formally test whether age, language family, syntactic marking, stimulus type, etc., account for effect-size variation above sampling error. 
We therefore use statistical meta-analysis to estimate the overall effect size of the shape bias. In addition, we address the four research questions above by coding variation in age, language/culture, stimuli/procedure, and syntactic framing. We then conduct a series of meta-regressions, testing whether these covariates moderate the estimated shape bias effect. 

At the outset, we first acknowledge that those moderators are not mutually exclusive, and we do not attempt to disentangle them. Secondly, we may not have enough data to fully test all of these moderators, so this would in itself be a useful finding speaking to how much empirical support there is for extant theoretical accounts. Given these limitations, moderator analyses should be interpreted as quantifying the degree to which the existing evidence can support claims about systematic variation, not as definitive tests of theoretical predictions.
We also report the results of all analyses attempted, as they were pre-registered before conducting the meta-analysis and we think it is important to highlight this. 
Finally, we also assess the degree of publication bias in the literature, and whether this bias is related to the reported magnitude of the shape bias in published data.

# Methods

```{r include=FALSE}
df_shape <- read_csv(here("data","shape_bias_meta.csv")) |>
  filter(!is.na(d))

df_shape$extraction[df_shape$extraction == "t_test"] <- "t-test"
df_shape$extraction[df_shape$extraction == "paper"] <- "table"
df_shape$englishgrp <- fct_relevel(as.factor(df_shape$language %in% 
                                                 c("english")), 
                                     "TRUE")


df_shape$mean_age_months_centered36 <- df_shape$mean_age_months - 36
df_shape$log_mean_age_months <- log(df_shape$mean_age_months)

# df_shape$indoeuropean <- fct_relevel(as.factor(df_shape$language %in% 
#                                                  c("english","spanish", "german")), 
#                                      "IndoEuropean", "Non-Indoeuropean")

df_shape <- df_shape %>% mutate(indoeuropean = case_when(df_shape$language %in% 
                                                 c("english","spanish", "german") ~ "Indo-European", TRUE ~ "Non-Indo-European"))

df_shape <- df_shape |>
  filter(!is.na(d), !is.na(indoeuropean), !is.na(mean_age_months_centered36))

df_shape <- df_shape %>% mutate(index = 1:length(d))
```

```{r eval=FALSE, echo=FALSE, include=FALSE}
# how many studies per language in a plot 
df_shape %>%
  group_by(language) %>%
  summarize(count = n()) %>%
  ggplot(aes(x=reorder(language, -count), y=count)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  labs(x="Language", y="Number of effect sizes") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r include=FALSE}

n_bilingual <- sum(df_shape$monolingual == "bilingual", na.rm=TRUE)
n_atypical <- sum(df_shape$child_type1 == "ASD", na.rm=TRUE)

df_shape <- filter(df_shape, 
                   monolingual == "monolingual", 
                   child_type1 == "TD", 
                   d<5)
```

```{r main_model, include=FALSE}
mod <- rma.mv(yi = d, 
              V = d_var, 
              random = ~ 1 | ID/exp_num/index,
              slab = short_cite, 
              data = filter(df_shape, !is.na(exp_num)))

mod_2lvl <- rma.mv(yi = d, 
              V = d_var, 
              random = ~ 1 | ID/exp_num,
              slab = short_cite, 
              data = filter(df_shape, !is.na(exp_num)))

mod_2lvl <- rma.mv(yi = d, 
              V = d_var, 
              random = ~ 1 | exp_num/index,
              slab = short_cite, 
              data = filter(df_shape, !is.na(exp_num)))

df_forest <- tibble(short_cite = "Meta-analytic estimate", 
                    d = coef(mod), 
                    ci.lb = mod$ci.lb,
                    ci.ub = mod$ci.ub)

#anova(mod, mod_2lvl)

```

```{r heterogeneity, include=FALSE}
#caluculating the heterogeneity from this website: https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate 

W <- diag(1/mod$vi)
X <- model.matrix(mod)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
I2 <- sum(mod$sigma2) / (sum(mod$sigma2) + (mod$k-mod$p)/sum(diag(P)))

# W <- diag(1/mod_2lvl$vi)
# X <- model.matrix(mod_2lvl)
# P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
# I2 <- sum(mod_2lvl$sigma2) / (sum(mod_2lvl$sigma2) + (mod_2lvl$k-mod_2lvl$p)/sum(diag(P)))


# using dmetar package for calculating I2 is only for three level modeling, mine is four level
```

```{r include=FALSE}
df_shape_solid <- df_shape %>% 
  filter(solid != "substance")
df_shape_summary <- df_shape %>%
  group_by(language) %>%
  summarize(count = n())

df_shape %>% group_by(complexity, ID) %>% filter( complexity == "complex") %>% summarize ( counts = n())
#length(which(df$column_name==value))

```

Except where noted, all hypotheses, literature search criteria, and statistical analyses were pre-registered at [http://tinyurl.com/shapebias]().

## Literature search

```{r include= FALSE, eval=FALSE}
prisma(found = 156,
       found_other = 22,
       no_dupes = 170, 
       screened = 170 , 
       screen_exclusions = 51, 
       full_text = 119,
       full_text_exclusions = 48,  # 39 + 9 papers not coded 
       qualitative = 71,
       quantitative = 71)

```

We created an initial set of papers to screen for inclusion in the meta-analysis. We based this set on the combined output of google scholar searches using the key words "shape bias", "word generalization", "word learning" combined. We also performed forward citation searches of two foundational papers in this literature: @landau1988 and @imai1997 and added a group of studies identified through domain knowledge and checking of reference lists. Figure \ref{fig:prisma} shows a PRISMA diagram describing our screening process. We only included published papers, excluding unpublished theses and dissertations.

In total, 170 papers were found. We filtered these first via titles and abstracts and then via full-text screening, based on the following eligibility criteria:
1. Effects must be from an experiment (e.g., with random assignment of participants to at least two conditions), 
2. The experiment must include participants less than or equal to 5 years old,
3. The experiment must use a word extension task that contrasts shape with other properties of the referent. 

Applying these criteria resulted in `r length(unique(df_shape$ID))` papers that both satisfied our criteria and reported enough information to calculate an effect size (ES) relating to one or more experimental conditions. An ES could be directly reported or could be computed based on reporting of the proportion of choosing shape in text, tables, graphs; or reporting a statistical test (typically a comparison of shape bias against chance using a test such as a one sample $t$-test). 

## Coding of effect size and moderators

The shape bias is typically assessed through word extension tasks [@landau1988]. In this task, children are shown a novel object labeled with a novel noun such as a "dax", presented with test objects varying in shape, and asked if each is a "dax." Most studies use either forced choice or endorsement tasks where children judge objects based on category membership.

We coded the effect size for each experimental condition as the standardized mean difference (Cohen's $d$) between the proportion of children who extended the label to shape-matching objects compared with other properties (e.g., color, texture, or material). In the case of forced-choice tasks, the effect size was calculated against chance, depending on the number of choices. In the case of endorsement tasks, the effect size was calculated as the difference between the proportion of children who extended the label to shape-matching objects compared with other properties (e.g., color, texture, or material).

We coded the variance of each effect size using the formula for Cohen's $d$ [@higgins2003].
Because many of the papers we coded were decades old, reporting standards varied considerably, and effect size extraction was often a challenge. Additionally, because studies often included multiple age groups, different types of stimuli, and various manipulations, one experiment usually yielded more than one effect size, and often papers included multiple experiments. Estimation of effect sizes followed this order of precedence: If the paper reported Cohen's d, we used it directly (N=`r sum(df_shape$extraction == "reported")`). If the Cohen's d was not reported, we looked for test statistics (N=`r sum(df_shape$extraction == "t-test")`). In the absence of this information, we used proportions reported in text and tables (N=`r sum(df_shape$extraction == "table")`), or graphs (N=`r sum(df_shape$extraction == "graph")`) along with the standard deviation SD or the standard error SE. In the absence of reported SD or SE, we computed these via standard formulae.

For each effect size, we attempted to code the following moderators: participant age; number of participants; type of syntax used -- informative (count/mass) or neutral^[We additionally coded whether common or proper noun syntax was used and (if applicable) whether animate or inanimate syntax was used, but both of these were sparsely used and we do not discuss them further.]; alternative test objects properties (e.g., shape, color, material); nature of the stimuli: solidity, animacy, and whether they were two- or three-dimensional; type of exposure before testing (whether the child saw multiple training examples of the target object, or just a single example); child's vocabulary size; country of test and language spoken by participants; response mode (grasping, looking, pointing, or verbalizing); measured behavior (e.g., behavioral, eye tracking); population type (typically developing or not); total number of trials. (Effect sizes are estimated across different groups of contrasting properties: shape vs color, material, function etc.). For most of these moderators, coding relied on explicit information reported and described in the paper. When the moderator was not explicitly mentioned (e.g., solidity of objects, syntax), we made inferences based on the stimuli and procedure descriptions provided when possible, otherwise we coded them as "null". Coding was performed by the first author, and a second coder independently coded a random subset of 15 papers (21% of the total) with 80% agreement. Discrepancies were resolved via discussion.

For our preregistered analyses, we excluded effect sizes from clinical populations (N=`r n_atypical`) and multilingual populations (N=`r n_bilingual`), because they are known to be very heterogeneous, though these effect sizes remain in the data file we provide.
Representation across languages varied substantially, such that there were `r sum(df_shape$language == "english")` effect sizes for English while other languages included only `r sum(df_shape$language != "english")` data points. Other languages included German(`r sum(df_shape$language == "german")`), Spanish(`r sum(df_shape$language == "spanish")`), Chinese(`r sum(df_shape$language == "mandarin")`), Japanese(`r sum(df_shape$language == "japanese")`), Korean(`r sum(df_shape$language == "korean")`), Vietnamese(`r sum(df_shape$language == "vietnamese")`), and Tsimane(`r sum(df_shape$language == "tsimane")`). 
The final coded sample consisted of `r length(unique(df_shape$ID))` papers with `r nrow(df_shape)` effect sizes. 

``` {r age_trend, include=FALSE}
df_shape_solid <- filter(df_shape, !is.na(exp_num), 
                         solidity == "solid")

df_shape_solid <- df_shape_solid %>% mutate(index = 1:length(d))


mod_intercept <- rma.mv(d ~ 1, 
                        V = d_var,
                        random = ~ 1 | ID/exp_num/index, 
                        slab = short_cite, 
                        data = df_shape_solid) 


mod_age <- rma.mv(d ~ mean_age_months_centered36, 
                  V = d_var,
                  random = ~ 1 | ID/exp_num/index, 
                  slab = short_cite, 
                  data = df_shape_solid)

mod_log_age <- rma.mv(d ~ log_mean_age_months, 
                      V = d_var,
                      random = ~ 1 | ID/exp_num/index, 
                      slab = short_cite, 
                      data = df_shape_solid) 

mod_poly <- rma.mv(d ~ I(mean_age_months_centered36^2), 
                   V = d_var,
                   random = ~ 1 | ID/exp_num/index, 
                   slab = short_cite, 
                   data = df_shape_solid)

polycoeff <- tibble(coeff = c(round(coef(mod_intercept),3)))

AICs <- tibble(aic = c(summary(mod_intercept)$fit.stats[5,'REML'],
                       summary(mod_age)$fit.stats[5,'REML'], 
                       summary(mod_log_age)$fit.stats[5,'REML'],
                       summary(mod_poly)$fit.stats[5,'REML']), 
               model = c("intercept", "age", "logage", "polyage")) |>
  mutate(delta_aic = aic - max(aic))

W_solid <- diag(1/mod_intercept $vi)
X_solid <- model.matrix(mod_intercept )
P_solid <- W_solid - W_solid %*% X_solid %*% solve(t(X_solid) %*% W_solid %*% X_solid) %*% t(X_solid) %*% W_solid
I2_solid <- sum(mod_intercept $sigma2) / (sum(mod_intercept $sigma2) + (mod_intercept $k-mod_intercept $p)/sum(diag(P_solid)))

W_poly <- diag(1/mod_poly $vi)
X_poly <- model.matrix(mod_poly )
P_poly <- W_poly - W_poly %*% X_poly %*% solve(t(X_poly) %*% W_poly %*% X_poly) %*% t(X_poly) %*% W_poly
I2_poly <- sum(mod_poly $sigma2) / (sum(mod_poly $sigma2) + (mod_poly $k-mod_poly $p)/sum(diag(P_poly)))

```

```{r forest, fig.height = 5, fig.width= 3 , echo=FALSE, fig.cap="Forest plot of all coded effect sizes, sorted by average effect size. Multiple points for an individual paper indicate multiple effects. Solid line is zero effect size, dashed line is the overall meta-analytic effect size. Color indicates language family.", fig.pos = "H", fig.align='center'}

#, out.width="100%"

df_shape %>% 
  mutate(short_cite = fct_reorder(short_cite, desc(d))) %>%
  ggplot(aes(x = short_cite, y = d, 
             ymin=d-sqrt(d_var)*1.96,
             ymax=d+sqrt(d_var)*1.96)) + 
  geom_pointrange(aes(color=indoeuropean), alpha = .5, position=position_dodge2(width=.5), size = 0.1) +
  geom_pointrange(data = df_forest, 
                  aes(ymin = ci.lb - 0.3, ymax = ci.ub + 0.3), pch = 5, col = "black", size = 0.2) + 
  coord_flip() +
  geom_hline(yintercept = 0, size = 0.2) +
  geom_hline(data = df_forest ,yintercept = df_forest$d , lty = 2 ) +
  aes(x=reorder(short_cite,-d, sum)) +  
  ylab("Standardized Mean Difference (d)") + 
  xlab("Citation") +
  ggthemes::scale_color_solarized(name = "Indo-European") +
  theme_minimal(base_size = 6) +
  # theme(legend.position = c(0.8, 0.5), legend.title=element_blank(), axis.title.y =element_blank(), 
  #       legend.background = element_rect(color = NA)) + 
  theme(legend.position = "bottom", legend.title=element_blank()) +
  guides(color = guide_legend(override.aes = list(size = 0.1))) 

#c(0.1, 0.25)
```
## Analytic approach

All analyses were performed using the `metafor` package in R [@viechtbauer2010]. We pooled effect sizes using a multi-level meta analysis model with effect size nested within experiment number, in turn nested within paper ID as a random effect. This complex random effect structure was necessary due to the non-independence of effect sizes from the same experiment and/or paper, since these experiments would typically share many characteristics (e.g., measure, procedure, or stimuli). We attempted to perform confirmatory analysis of our preregistered hypotheses via multi-level meta-regressions including age, language, item properties of the stimuli, and syntax as fixed effects. In some cases (described below), we did not have sufficient data to add particular factors to our analyses.

# Results

Figure \ref{fig:forest} shows the forest plot for coded effect sizes. Our initial meta-analytic model with no moderators revealed an overall effect size of `r round(coef(mod), 2)` [`r round(mod$ci.lb, 2)`, `r round(mod$ci.ub,2)`] ($p$ `r papaja::apa_p(mod$pval)`). Nevertheless, effects varied substantially, yielding a substantial amount of within- and between-study heterogeneity ($I^2 = `r round(I2, 2)`$). 
Not all stimuli in our coded studies were expected to produce a shape bias, with `r length(which(df_shape$solidity == 'solid')) ` effect sizes involving solid objects and only `r length(which(df_shape$solidity != 'solid'))` using non-solid objects. Although we did not preregister removing studies with non-solid targets, we believe this is an important step since all theories of the shape bias would predict smaller effects for non-solid targets. In our remaining analysis, we focus on the subset of effects using solid objects, deviating from our preregistration. Heterogeneity for this subset was $I^2 = `r round(I2_solid, 2)`$. So, the heterogeneity remains high even when generalizing by shape for a set of stimuli with only solid objects, motivating the moderation analyses below. 

## Developmental change

```{r include=FALSE}

mod_ployage_ie <- rma.mv(d ~ indoeuropean, 
                         V = d_var,
                         random = ~ 1 | ID/exp_num/index, 
                         slab = short_cite, 
                         data = df_shape_solid)

#anova(mod_poly , mod_ployage_ie, refit=TRUE)

poly_predictions <- as.data.frame (predict(mod_ployage_ie))
poly_predictions$short_cite <- df_shape_solid$short_cite
vector1 <- 1:nrow(df_shape_solid)
poly_predictions$index <- vector1
# df_shape_solid$index <- vector1
merged <- merge(x = poly_predictions, 
                y = df_shape_solid[ , c("mean_age_months", "indoeuropean",
                                        "index")], by = "index")


W_age_lang <- diag(1/mod_ployage_ie $vi)
X_age_lang <- model.matrix(mod_ployage_ie )
P_age_lang <- W_age_lang - W_age_lang %*% X_age_lang %*% solve(t(X_age_lang) %*% W_age_lang %*% X_age_lang) %*% t(X_age_lang) %*% W_age_lang
I2_age_lang <- sum(mod_ployage_ie $sigma2) / (sum(mod_ployage_ie $sigma2) + (mod_ployage_ie $k-mod_ployage_ie $p)/sum(diag(P_age_lang)))

```

Due to the high overall heterogeneity, we investigated the influence of the developmental changes in the magnitude of the shape bias by adding age as a factor. Model selection followed our preregistered strategy of comparing four types of functional forms in our multi-level model: constant, linear, logarithmic, and quadratic, using the corrected Akaike information criterion AICc. A difference of $\Delta_{AIC} > 4$ between the lowest AICc and any other AICc was interpreted as a meaningful difference [@Burnham2004; @burnham2013model]. 

All models including age didn't fit significantly different than the model without age, and didn't meet our pre-registered criteria ($\Delta_{AIC}$ between `r round(AICs$delta_aic[AICs$model == "age"], 2)` for linear and `r round(AICs$delta_aic[AICs$model == "polyage"], 2)` for polynomial) to justify model's complexity. So, we adopted a constant function for our subsequent analysis with a significant overall estimate of `r round(polycoeff$coeff[1], 3)` ($p=$ `r papaja::apa_p(mod_intercept$pval)[1]`) and a heterogeneity of ($I^2 = `r round(I2_solid, 2)`$).

## Cross-linguistic differences

Based on the previous literature, we hypothesized that language and culture would be potential moderators influencing the degree of shape-based generalization. However, due to limited data availability beyond English, conducting language-specific analyses was not feasible. As a consequence, languages were categorized into two groups: Indo-European (encompassing German and Spanish and English), Non-Indo-European (including Japanese, Mandarin, Vietnamese, Korean, and Tsimane). We acknowledge that this grouping is not ideal, as it might oversee some linguistic and cultural diversity, for example, Japanese and Tsimane being in the same group despite the vast cultural and environmental differences. However, the set of languages represented in the available studies is too limited and heterogeneous to support other types of theoretically informative categorization. Thus, we use this IE vs. non-IE grouping strictly as a pragmatic proxy to compare English with studies conducted in other linguistic communities.

```{r include = FALSE, results = 'asis'}

#contrasts(df_shape$indoeuropean) <- contr.sum

lang_model <- rma.mv(d ~ indoeuropean, 
                     V = d_var,
                     random = ~ 1 | ID/exp_num/index, 
                     slab = short_cite, 
                     data = df_shape_solid)


lang_model_all <- rma.mv(d ~ language, 
                     V = d_var,
                     random = ~ 1 | ID/exp_num/index, 
                     slab = short_cite, 
                     data = df_shape_solid)

mdlcoeff <- tibble(coeff = c(coef(lang_model)))
#mdlcoeff <- tibble(coeff = c(round(coef(lang_model),2)))
#mdlcoeff$coeff[1]


W_lang <- diag(1/lang_model$vi)
X_lang <- model.matrix(lang_model)
P_lang<- W_lang - W_lang %*% X_lang %*% solve(t(X_lang) %*% W_lang %*% X_lang) %*% t(X_lang) %*% W_lang
I2_lang <- sum(lang_model$sigma2) / (sum(lang_model$sigma2) + (lang_model$k-lang_model$p)/sum(diag(P_lang)))


lang_model_df <- as.data.frame(round(coef(summary(lang_model)), 4))

kableExtra::kbl(lang_model_df) %>%
  kable_classic_2(full_width = F)

writeLines(knitr::kable(lang_model_df,
                        format = "latex", 
                        digits = 3,
                        table.attr = "style='width:10%;'"))

summary(lang_model)

```


\begin{table}[t]
\centering
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{lrrrrrr}
\hline
  & \bf{$\beta$} & \bf{SE} & \bf{$z$} & \bf{$p$} & \bf{lower CI} & \bf{upper CI}\\
\hline
Intercept & 0.63 & 0.1 & 6.13 & <.0001 & 0.43	& 0.83\\
Non-Indo-European & -0.23 & 0.14 & -1.59 & 0.11 & -0.51 & 0.005\\
\hline
\end{tabular}
}
\caption{\label{langmodel} Results of the meta-regression model incorporating both language and age (in months). Non-IE = "Non-Indo-European."}
\end{table}

A model incorporating language group as a moderator revealed a significant coefficient of $\beta=$ `r round(mdlcoeff$coeff[1], 2)` ($p=$ `r papaja::apa_p(lang_model$pval)[1]`) for Indo-European populations. The effect was negative but not significantly so for the non-Indo-European group: $\beta=$ `r round(mdlcoeff$coeff[2], 2)` ($p=$ `r papaja::apa_p(lang_model$pval)[2]`). The effect showed a between-study heterogeneity remained high in this moderated model ($I^2 = `r round(I2_lang, 2)`$), with most of the variance at the between-paper level ($sigma^2$ = `r round(lang_model$sigma2[1],3)`). All coefficients are shown in Table \ref{langmodel}, and Figure \ref{fig:dev-change} shows model fit compared with individual effect size estimates. 
```{r dev-change, fig.width = 8, fig.height= 4, out.width = "100%", fig.cap = "Effect size plotted by average participant age. Points indicate individual effect, and error bars show variance. Points are colored based on language, and model fits for Indo-European and non-Indo-European languages are shown in black.", fig.pos = "H"}
ggplot(df_shape_solid,
       aes(x = mean_age_months, y = d, color = language)) +
  geom_pointrange(aes(ymin = d - d_var, ymax = d + d_var),
                  alpha = .5, size = 0.1) +
  geom_line(data = merged, 
            aes(y = pred),
            group = 1, 
            col = "black") +
  geom_hline(yintercept = 0, lty = 3) + 
  ylab("Standardized Mean Difference (d)") + 
  xlab("Mean age (months)") +
  scale_color_discrete(name = "Language") +
  facet_wrap(~indoeuropean) + 
  theme_minimal() + 
  theme(legend.position = "bottom")

ggsave("figs/dev_change_lang.png", width = 6, height = 4, units = "in", dpi = 300)

# ggplot(df_shape_solid, aes(x = mean_age_months, y = d, color = language)) +
#   geom_pointrange(aes(ymin = d - d_var, ymax = d + d_var), alpha = .5, size = 0.1) +
#   geom_line(data = merged, aes(y = pred), group = 1, col = "black") +
#   geom_ribbon(
#     data = merged,
#     aes(x = mean_age_months, ymin = pred - ci.lb, ymax = pred + ci.ub), # Add x aesthetic here
#     fill = "darkgray",
#     alpha = 0.2,
#     inherit.aes = FALSE
#   ) +
#   geom_hline(yintercept = 0, lty = 3) +
#   ylab("Standardized Mean Difference (d)") +
#   xlab("Mean age (months)") +
#   scale_color_discrete(name = "Language") +
#   facet_wrap(~indoeuropean) +
#   theme_minimal() +
#   theme(legend.position = "bottom")

```

```{r include = FALSE}
df_shape_indo <- df_shape_solid %>%
  filter(indoeuropean == TRUE)

df_shape_nonendo <- df_shape_solid %>% 
  filter(indoeuropean == FALSE)

```
The pioneering cross-linguistic work on the shape bias has been instrumental in shaping the field’s theories, particularly by showing that the patterns observed in English are not always universal. At the same time, our systematic search revealed that the literature as a whole has not yet achieved the breadth needed for a large-scale quantitative comparison across many languages. The significant imbalance in the data, with a predominance of studies on English, meant we were underpowered to conduct a fine-grained cross-linguistic analysis. Our findings should therefore not be taken as evidence against cross-linguistic differences, but rather as a call to build upon the foundational non-English studies included in our review. Expanding this research to a wider and more diverse range of languages and cultures is a critical next step for the field.

## Shape and complexity effects

<!-- * RQ3: The strongest shape bias effect will be observed with simple uniform solid objects.  -->
We also hypothesized that the complexity of solid objects might moderate shape bias. However, only `r length(unique(df_shape$ID[df_shape$complexity == "complex"]))` papers with `r length(which(df_shape$complexity == "complex"))` effect sizes in our sample reported complexity as a dimension of interest and explicitly classified objects along this dimension. Thus, assessing the impact of object complexity was impractical due to sparse data, variations in stimulus types across studies, along with unclear criteria for categorizing objects as complex or simple, posing challenges for a comprehensive analysis. 

Early studies examining the role of object properties like complexity and function were groundbreaking and opened up important theoretical avenues for a mechanistic explanation of the shape bias behaviour. Our meta-analysis affirms the continued importance of this topic and highlights the rich and varied ways this construct has been operationalized in the literature, often being intertwined with functionality, a collective challenge the field has faced. 

## Syntactic effects

<!-- * RQ4: Lastly, count and mass syntax will moderate the magnitude of the shape bias for younger children but not older children.   -->
``` {r include = FALSE}
# polynomial
syntax_model <- rma.mv(d ~ Informative_syntax,
       V = d_var, 
       random = ~ 1 | ID/exp_num/index,
       slab = short_cite, 
       data = df_shape_solid)

mdlsyntaxcoeff <- tibble(coeff = c(round(coef(syntax_model),2)))

count_mass_df <- df_shape_solid %>% filter(Informative_syntax != "neutral" & count_mass != "not mentioned" & count_mass != "both")

count_model <- rma.mv(d ~ count_mass, 
       V = d_var, 
       random = ~ 1 | ID/exp_num/index,
       slab = short_cite, 
       data = count_mass_df)

countmdlcoeff <- tibble(coeff = c(coef(count_model)))

```
Lastly, we hypothesized that the use of count/mass noun syntax marking would moderate the extent of the shape bias in younger ages. A model that incorporated the nature of syntactic marking (informative or neutral) indicated no significant effect of the presence of informative syntax ($p=$ `r papaja::apa_p(syntax_model$pval)[2]`). The majority of studies in our sample used informative syntax, with `r length(which(df_shape_solid$Informative_syntax == "neutral"))` of `r length(df_shape_solid$Informative_syntax)` effect sizes using neutral syntax. Thus, we next fit a model with only the subset of the data that used informative syntax, comparing count to mass syntax. Although with a negative coefficient of mass syntax $\beta=$ `r round(countmdlcoeff$coeff[2], 2)`, this model did not yield a significant effect of count vs. mass ($p=$ `r papaja::apa_p(count_model$pval)[2]`). 

Syntactic bootstrapping theories have provided an influential framework for understanding how grammatical cues might support word learning. Our meta-analysis contributes to this conversation by examining the effect of informative count and mass noun syntax across dozens of studies. Despite the statistical non-significance, interpretation of this result is not straightforward given that even this subset of experiments that compared count and mass syntax still carried over the general imbalance in the data. If this manipulation was predominantly used with English speaking children, and only `r (length(which(df_shape_solid$mean_age_months < 24)) / nrow(df_shape_solid) ) *100`% of our data came from younger than two years old children (with the assumption that this age group could be the least sensitive to syntax), then previous exposure to this syntactic distinction may have influenced the results. Additionally,  The result—a non-significant effect of syntactic marking—suggests that the influence of these cues, when aggregated, may be more nuanced than previously understood. which raises a question for future research: under what specific conditions are syntactic cues most influential, and how do they interact with the host of other factors present in a word-learning task? Our work suggests this is a fruitful area for further investigation.

## Publication bias

``` {r eggers test copied from its source code, eval = FALSE}
eggers.test = function(x) {

    # Validate
    x = x

    if (x$k < 10) {

        warning(paste("Your meta-analysis contains k =",
                      x$k, "studies. Egger's test may lack the statistical power to detect bias when the number of studies is small (i.e., k<10)."))

    }

    if (class(x)[1] %in% c("meta", "metabin", "metagen", "metacont", "metacor", "metainc", "metaprop")) {

        # Conduct metabias
        eggers = meta::metabias(x, k.min = 3, method = "linreg")

        # Get Intercept
        intercept = as.numeric(eggers$estimate[1])

        # Get SE
        se = as.numeric(eggers$estimate[2])

        # Calculate 95CI
        llci = intercept - qnorm(0.975) * se
        ulci = intercept + qnorm(0.975) * se

        # Get t
        t = as.numeric(eggers$statistic)

        # Get df
        df = as.numeric(eggers$parameters)

        # Get p
        p = as.numeric(eggers$p.value)

        # Make df
        returnlist = list(intercept = intercept,
                          llci = llci,
                          ulci = ulci,
                          t = t,
                          p = p,
                          meta.obj = x)

    } else {

        stop("x must be of type 'metabin', 'metagen', 'metacont', 'metainc' or 'metaprop'")

    }

    class(returnlist) = "eggers.test"

    return(returnlist)

}
```

``` {r include = FALSE}

eggerstest <- df_shape_solid %>%
  mutate(y = d/d_var, x = 1/d_var) %>%
  lm(y ~ x, data= .) %>%
  summary()

eggerscoeff <- tibble(coeff = c(round(coef(eggerstest),2)))
```
One potential source of bias in meta-analysis comes from publication bias, the tendency for positive findings (e.g., effects that are significant at $p < .05$) to be included in papers both submitted and accepted for publication. We evaluated publication bias using two standard meta-analytic diagnostics. We first created a significance funnel plot (depicted in Figure \ref{fig:funnelplot}) following @mathur2020 to visualize the relationship between effect size and sampling variance; this type of plot highlights positive selection biases. This plot showed more positive studies with large standard errors than negative studies, consistent with a publication bias in which noisy studies with positive effects are more likely to appear in the literature. We also complemented the visual inspection by employing an Egger's regression test of small-study effects as a quantitative measure of asymmetry. The results yielded a statistically significant positive intercept and slope ($\beta0=$ `r round(eggerscoeff$coeff[1], 2)`, $\beta1=$ `r round(eggerscoeff$coeff[2], 2)`, $p< 0.0001$), suggesting that smaller studies tend to report larger effects and affirming the presence of asymmetry.

However, it is important to note that these diagnostics are not definitive proof of publication bias, as other factors such as true heterogeneity among studies or methodological differences can also lead to similar patterns. Therefore, while our findings suggest the possibility of publication bias, they should be interpreted with caution and in the context of the broader literature.

# Discussion

In this paper, we were interested in taking stock of evidence for the shape bias, as well as quantifying its variation across cultures, ages, and experimental conditions.
Synthesizing effect sizes across studies provided evidence of a positive pooled shape bias estimate across nearly all ages and languages studied.
However, the synthesis also revealed no significant cross-linguistic/cultural differences, and substantial between-study heterogeneity. The preponderance of large effect sizes from small studies in our data suggests there may be a bias to publish effects favoring the existence of a shape bias. Drawing definitive conclusions about this kind of publication bias is challenging, yet this publication bias -- as well as procedural variation across studies -- may contribute to the extremely high between-study heterogeneity we observed. below we discuss these findings in the context of theoretical accounts of the shape bias.
```{r funnelplot, fig.width= 4, fig.height= 3, include=TRUE, fig.cap="The significance funnel plot shows individual effect sizes on the x-axis and corresponding standard error on the y-axis. The diagonal line shows effect sizes that are exactly p = 0.05. Yellow values show affirmative (significant) studies. The diamond shows the estiamte from the meta-analytic model described above.", fig.align = "center", fig.pos = "H"}

#linear model
PublicationBias::significance_funnel(yi = df_shape_solid$d,
                                     vi = df_shape_solid$d_var,
                                     est_all = coef(mod_age)[1]) +
  theme(legend.position = "bottom")

```

Statistical generalization theories of shape bias have two proposed (not mutually exclusive) sources of this generalization: lexical statistics and environmental regularities. Lexical statistics studies tend to focus on early English vocabulary, while the primary investigation of environmental regularities compared US and Tsimane groups in Bolivia. Direct examination of these statistical regularities hypotheses in the current meta-analysis was impeded, however, by limited data availability on the lexical statistics, early vocabulary composition, and the distribution of shape-based objects in other communities and languages. The number of studies reporting vocabulary size (corresponding to N=`r sum(! is.na(df_shape_solid$Vocabulary))` effect sizes) was too small to be included in the analysis. Additionally, The interchangeable nature of language and location variables in our dataset (e.g., English predominantly corresponds to the US, and Japanese to Japan), as well as the potential role of syntax in forming associations, making it challenging to differentiate the influence of these various factors [@gershkoff-stowe_shape_2004; @samuelson1999; @jara2022; @imai1997; @barner_language_2009]. 

Although we are hesitant to over-interpret developmental trends given the heterogeneity we observed, but when developmental data is aggregated, the analysis indicates that a constant model provides the best fit, suggesting a relatively stable shape bias across the tested ages (min=`r min(df_shape_solid$mean_age_months)`, max=`r max(df_shape_solid$mean_age_months)` month-old). This result presents a more complex picture than often assumed, adding a new layer of nuance to theories that emphasize the statistical learning of regularities in the input as a source of the bias, or the strengthening of it with age and lexical growth. It's important to interpret this finding in the context of the literature as a whole. As noted by [@cao_lewis_2024], aggregated developmental data can be influenced by factors like age-specific task calibrations, or a bias towards publishing those inflated effect sizes from younger children, that make it challenging to isolate a pure developmental signal. This result suggests that task specifics and stimuli may exert a strong influence over the outcome of the studies in ways unintended by the experimenters, underscoring the need to examine the reliability and validity of the measures employed [@cimpian2005absence ; @imai1997; @samuelson_dynamic_2007]. For instance, the data in some studies might be contaminated by the use of familiar stimuli. Some studies use novel objects with novel labels, others use familiar objects with novel labels; both make claims about early label based category induction with different age groups. If familiar objects were used to test relatively older children (4 or 5 years old), these children can utilize their prior knowledge and experience to categorize objects, not solely relying on the saliency of perceptual properties. Thus, while individual studies have been essential in mapping parts of the developmental story, our synthesis suggests that when viewed collectively, the developmental trend is less pronounced than the variability between study procedures.
<!-- shape bias is not a monolithic phenomenon, but rather a complex interplay of multiple factors, including age, language, and experimental conditions. -->

Empirical efforts to distinguish the shape bias at the phenomenon level from the mechanistic level has involved changing many parameters that are of theoretical interest to the specific hypothesis being tested. An example of this is the investigation of the influence of conceptual information and functionality on word meaning acquisition and the nature of hypothesis shift the child undergoes to relinquish shape bias in favor of other bases. In this subdivision of the literature, functionality has been closely related to complexity [@imai1997;@cimpian2005absence], with both terms used interchangeably. The current meta-analysis highlights the need for more nuanced investigation of both dimensions to adjudicate them. In addition, despite that developmental trajectories could potentially present an indicator of the mechanism underlying a behavior, the stable trend in this dataset combined with the heterogeneity and lack of moderation power 1- support neither a fully learned account, nor an early emergent account that is stabilized by consistent regularities in the input and 2- doesn't conclude whether children use perceptual cues like shape in an automatic attentional process, or as a strategic cue in a more flexible process to look for object kinds.

In addition, the presence of a publication bias has significant implications for adjudicating between theoretical accounts of the shape bias. For example, theories relying on specific moderators (e.g., the effect of informative syntax or object complexity) may be challenging to interpret in light of biases in the literature. Small, under-powered studies designed to test these moderators may only be published if they find a significant main effect of shape, regardless of the outcome of the moderation analysis. This practice would obscure true null findings and create a literature that makes it difficult to disentangle procedural artifacts from genuine theoretical constraints. It is important to note that because the shape-bias literature encompasses multiple paradigms that differ in sample sizes, stimuli, and coding practices, classical publication-bias tests cannot isolate selective reporting from genuine methodological heterogeneity. We therefore emphasize that our results are suggestive rather than definitive.

In sum, the current meta-analysis findings, including the pooled estimate, enables evidence-based power calculations for future studies, and the identification of gaps in the literature that need to be addressed. It also sheds light on challenges in using the empirical data to adjudicate between theories of the shape bias. We found that many studies used different types of stimuli, tasks, and measures, making it challenging to identify the potentially relevant factors and their interactions over different contexts within the same population, let alone across different populations. We also acknowledge that very high heterogeneity and inconsistent reporting constrain some inferences (e.g., fine-grained cross-linguistic moderators). Rather than treating this as a fatal flaw, we emphasize that quantifying heterogeneity and revealing reporting gaps is itself a major scientific contribution: it turns the field’s known concerns into measurable targets for improvement (e.g., harmonized stimuli, consistent definition, standardized reporting of vocabulary/syntax).
We believe this work highlights that cumulative progress on the theoretical questions behind the shape bias may require standardized materials, shared corpora/stimuli sets, preregistered multi-site designs, and coordinated reporting of key procedural variables to better explain this important aspect of early word learning. 

\newpage
# References

::: {#refs custom-style="Bibliography"}
:::
